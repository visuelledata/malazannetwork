---
title: "Malazan Network Analysis"
author: "Christopher Peralta"
date: "Friday, Apr 26, 2019"
output: 
  tufte::tufte_handout: default
  toc: true
editor_options: 
  chunk_output_type: console
urlcolor: blue
---

```{r setup, include=TRUE, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  echo = FALSE,
  warning = FALSE, 
  #out.width = '75%',
  #out.height = '75%', 
  comment = "#>", 
  #fig.keep = "last", 
  dpi = 600, 
  warning = FALSE, 
  message = FALSE, 
  dev = 'jpeg', 
  cache = TRUE)


# All packages available on CRAN
library(tidyverse)
library(ggraph)
library(tidygraph)
library(tidytext)
library(readr)
library(hrbrthemes)

# Read in data
network_data <- read_rds("data/network_data.rds")
pre_network_data <- read_rds("data/pre_network_data.rds")
pov_data <- read_rds("data/pov-data.rds")
char_appearances <- read_rds("data/char_appearances.rds")
occurrence_data <- read_rds("data/char_appearances.rds")
#network_data <- read_rds("data/network_data.rds")
source("R/import_books.R")
source("R/import_characters.R")
source("R/standardize_names.R")

#theme_set(theme_ipsum())

#source("gghelpers.R")

#ggplot <- function(...) ggplot2::ggplot(...) + scale_colour_manual(values = color) + #theme(plot.title = element_text(size = 11.5))
```

```{r numbers}
# characters %>% 
#   rename(names = name) %>% 
#   remove_NAs() %>% 
#   #standardize_names_net() %>% 
#   distinct(names) 

# pre_network_data %>% mutate(names = str_split(names, ";")) %>% unnest() %>% 
#   distinct(names) 

num_char <- function(){
  num_char_low <- as_tbl_graph(network_data, directed = FALSE) %>% 
    activate(nodes) %>%
    mutate(popularity = centrality_degree()) %>% 
    filter(popularity != 0) %>% 
    activate(nodes) %>% 
    pull(name) %>% 
    length() %>% 
    formatC(format="d", big.mark=",")
  
  num_char_high <- as_tbl_graph(network_data, directed = FALSE) %>% 
    activate(nodes) %>% 
    pull(name) %>% 
    length() %>% 
    formatC(format="d", big.mark=",")

  c(num_char_low, num_char_high)
}

num_words <- 
  books %>% 
  filter(!str_detect(line, "^Chapter .+|^CHAPTER .+|^[A-Z]+$|^[A-Z]+\\d{1,2}$")) %>% 
  unnest_tokens(words, line) %>% 
  pull(words) %>% 
  length() %>% 
  formatC(format="d", big.mark=",")


pov_characters <- pov_data %>% 
  distinct(name) %>% 
  mutate(name = str_remove_all(name, "\\(.+\\)"), 
         name = str_trim(name)) %>%
  distinct(name) %>% 
  pull(name)

num_pov <- pov_characters %>% 
  unique() %>% 
  length()
```

# Abstract
```{marginfigure, echo=TRUE}
Note: All bolded words are hyperlinks. 
```
In this project I attempt to find the climaxes of the series [**Malazan Book of the Fallen**](https://en.wikipedia.org/wiki/Malazan_Book_of_the_Fallen) by using network data and sentiment analysis. This series is notable in that it is one of the most complex and long fantasy series with a continuous single plotline. There are `r num_words` words in the series and I estimated that there are `r num_char()[[2]]` characters in the series with approximately `r num_pov` unique points of view. Additionally, many of the characters have multiple aliases and nicknames adding another layer of complexity. A character might go by completely different names in different novels. 

I began by mining the co-occurrence data from the ngrams; I used a combination of regular expressions, parallelization, and more. I'll go more into that process later on. From there, I had to get the co-occurrence data into a reasonable format and clean the name data. I then used the AFFINN Sentiment Lexicon to get the sentiment data. Finally, I compared the sentiment data with the network data to try to see which works better to determine the climax of each of the 10 books. 

My general conclusion was that edge betweenness centrality aggregated by chapter using the mean does the best job of finding the actual climaxes of each book.

# Introduction

The goal of this study was to use network and sentiment analysis to find the climax of each book in the series. I used numerous datasets from several sources in this project. The co-occurrence data was mined from the books by me. The books were converted from `.epub` format to `.txt` format. I made most of the `alias` data manually and crowdsourced some of the aliases on [**Reddit**](https://www.reddit.com/r/Malazan/comments/alaknz/spoilers_what_are_aliases_are_there_in_this_series/). The `name` data was manually extracted from the *Dramatis Personae* sections at the start of each book and manually extracted from the [**Malazan Wiki**](https://www.reddit.com/r/Malazan/comments/alaknz/spoilers_what_are_aliases_are_there_in_this_series/). 

To mine the character co-occurrence data, I started by converting `epub` versions of the novels to `.txt` files and reading them into R. Then I added book numbers, chapter numbers, and stripped the front and back matter from each book. From there, I turned the text into a series of ngrams of length 20 split by book and chapter. At this point, I wrote a function that extracts the characters' names from the ngrams by row and then puts the co-occurrence data into a workable format. Cleaning and formatting the co-occurrence data was the next step. Finally, I used a variety of methods to find the climaxes of each book. 

# Method and results

## Method
I'll begin this section with a detailed description of the methods and assumptions I used in mining the character names from the novels as that was the most difficult part of the project. 

I began by joining the character name data with the alias data into a single dataset. I then split all of this data by spaces in order to get variations of the names and rejoined the partial name data back to the full name data to get a comprehensive dataset of full and partial names. I then filtered out stop words, formal titles, military ranks, and commonly capitalized words that aren't names from this list. Then, I arranged the list by character length.

At this point, I went back to the book data and turned the text into ngrams of length 20 by book and chapter. I chose to use ngrams so that I would get the full co-occurrence relations within the 20 word groups. For example, "Ron Jon" then "Ron Jon Bob" and finally "Jon Bob". Then I wrote a function that tries to extract every name in the name list from the ngram. If there is a match, then it also removes the match from the string for subsequent iterations. Otherwise, "Brys Beddict" would be extracted 3 times. 

This method of extraction was incredibly computationally intensive as there were 3,080 names in my name list and 3,250,530 ngrams. My code went through multiple iterations, and I eventually added parallelization and broke my data into 263 chunks. All of this managed to get my code to run in around 40 hours on my laptop. The original speed was about 310 seconds per 1600 ngrams, and I got that down to about 70 seconds per 1600. These speeds are what I recall, initially I did not think to record them. Most of the improvement was due to the parallelization and an `if` statement before the `str_extract_all` call. I attached the code for this part in the appendix. 

Once I got the co-occurrence data into a usable format, I had a lot of data cleaning to do. A lot of the data cleaning was due to the fact that there were partial name matches due to the use of ngrams "John Smith" matched as "John", and due to the fact that some characters have up to 9 different names that they might go by. My job was made slightly easier by the fact that less important characters tend to have fewer names in the series, such as Mallet, Picker, or Antsy whose surnames are never revealed. I formatted and removed variations of all names with over 100 appearances in the network data. I used over 130 regular expressions to achieve this. I made three important assumptions at this stage: 

* The names with over 100 occurrences in the co-occurrence data in addition to the uncleaned names of the less featured characters are sufficient to fully represent the true co-occurrence network. 
*  The most common variations of names accurately represent the co-occurrence relationships of their specific character. 
* Any extremely uncommon name variations will be filtered out as isolated nodes or by a small filter removing the most uncommon nodes.
 
After cleaning the data, I began to analyze the co-occurrence data. 

## Results

I'll jump right into it by giving the top 10 most important characters by PageRank compared with their degree centrality importance. I only used undirected network graphs for everything that follows. 
```{r}
as_tbl_graph(network_data, directed = FALSE) %>% 
  mutate(pop_deg = centrality_degree(), 
         pop_pr = centrality_pagerank()) %>% 
  arrange(desc(pop_deg)) %>% 
  mutate(rank_degree = 1:1493) %>% 
  arrange(desc(pop_pr)) %>% 
  mutate(rank_pagerank = 1:1493) %>% 
  as.tibble() %>% 
  select(-pop_deg, -pop_pr) %>% 
  head(10) %>% 
  knitr::kable(col.names = c("Character", "Ranking - Degree Centrality", "Ranking - PageRank"))
```

Degree centrality and PageRank mostly agree on the most important characters in the series, but they start to greatly diverge the further they get away from the top 10. As shown in the figure to the right. 
```{r fig.margin=TRUE}
as_tbl_graph(network_data, directed = FALSE) %>% 
  mutate(pop_deg = centrality_degree(), 
         pop_pr = centrality_pagerank()) %>% 
  arrange(desc(pop_deg)) %>% 
  mutate(rank_degree = 1:1493) %>% 
  arrange(desc(pop_pr)) %>% 
  mutate(rank_pagerank = 1:1493) %>%
  ggplot(aes(rank_degree, rank_pagerank)) + 
  geom_line(color = "grey25") + 
  theme_ipsum()
```

In the following network graph, I used PageRank to calculate the centrality because I don't feel that degree centrality is as suitable for this network.  

```{r results='asis', fig.fullwidth=TRUE,out.width="450px"}
book_introduced <- pre_network_data %>%
  mutate(name = str_split(names, ";")) %>%
  unnest(name) %>%
  group_by(name) %>%
  summarize(book_introduced = as.factor(min(book))) %>%
  filter(!str_detect(name, "^ "))

mode_s <- function(x){
  unique_x <- unique(x)
  unique_x[which.max(tabulate(match(x, unique_x)))]
}

book_most_mentioned <-
  pre_network_data %>%
  mutate(name = str_split(names, ";")) %>%
  unnest(name) %>%
  group_by(name) %>%
  summarize(book_most_mentioned = as.factor(mode_s(book)))

# Code for full graph
pallette <-  RColorBrewer::brewer.pal(10, "Set3")
as_tbl_graph(network_data, directed = FALSE) %>%
  mutate(popularity = centrality_pagerank()) %>%
  arrange(desc(popularity)) %>% 
  mutate(rank = row_number()) %>% 
  filter(!node_is_isolated()) %>%
  activate(edges) %>% 
  mutate(book = as.integer(book)) %>% 
  arrange(book) %>%
  mutate(book = as.factor(book)) %>% 
  distinct(from, to, book, chapter) %>% 
  ggraph(layout = "nicely") +
  geom_edge_link(aes(color = book)) +
  geom_node_point(aes(size = popularity^2.5), color = "grey20") +
  theme_void() +
  theme(legend.position="none") +
  scale_color_manual(values = pallette)
  
cat('\n')
knitr::include_graphics("full-network-graph.png")
cat('\n')
#cat('\n![](full-network-graph.png)\n')
```

The only interesting observation I can make from this graph is that there is a clear distinction between the two main continents from the 4 first books in the series and the third main continent introduced in book 5. 

```{r}
sentiment_scores <- books %>% 
  unnest_tokens(word, line) %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(book, chapter) %>% 
  summarize(score = sum(score)) %>% 
  ungroup()

# char_score <- char_appearances %>% 
#   left_join(sentiment_scores) %>% 
#   group_by(names) %>% 
#   summarize(mean_score = mean(score, na.rm = TRUE)) %>% 
#   arrange(mean_score)
# 
# #OMIT
# as_tbl_graph(network_data, directed = FALSE) %>% 
#   mutate(popularity = centrality_pagerank()) %>% 
#   arrange(desc(popularity)) %>% 
#   left_join(char_score, by = c("name" = "names")) %>% 
#   as.tibble() %>% 
#   ggplot(aes(mean_score, popularity)) + 
#   geom_point(alpha = .15) +
#   theme_ipsum()
```

Before I begin trying to find the best way to find the climaxes in the series, I'll check if a chapter's sentiment score is corellated to it's importance, since I initially thought that either of them could be used to predict climaxes on their own. The importance of the chapters was calculated by taking the mean of the edge betweenness centrality values of all of the edges in each chapter. 

```{r}
as_tbl_graph(network_data, directed = FALSE) %>% 
  activate(edges) %>% 
  mutate(book = as.double(book), 
         chapter = as.integer(chapter)) %>% 
  mutate(importance = centrality_edge_betweenness()) %>% 
  as.tibble() %>% 
  select(book, chapter, importance) %>% 
  group_by(book, chapter) %>% 
  summarize(mean_importance = mean(importance)) %>% 
  ungroup() %>% 
  arrange(desc(mean_importance)) %>%
  left_join(sentiment_scores) %>% 
  ggplot(aes(score, mean_importance)) + 
  geom_point() + 
  theme_ipsum()
```

It appears that there is little to no correllation between a chapter's mean sentiment and mean edge betweenness centrality. 

Now, I'm going to try to see whether or not I can find the climaxes of the series using the mean edge betweenness centrality of each chapter weighted with the total sentiment score of each chapter. 

```{r}
importance_df <- as_tbl_graph(network_data, directed = FALSE) %>% 
  activate(edges) %>% 
  mutate(book = as.double(book), 
         chapter = as.integer(chapter)) %>% 
  mutate(importance = centrality_edge_betweenness()) %>% 
  as.tibble() %>% 
  select(book, chapter, importance) %>% 
  group_by(book, chapter) %>% 
  summarize(mean_importance = mean(importance)) %>% 
  ungroup()

importance_df %>% 
  left_join(sentiment_scores) %>% 
  mutate(importance_weighted = score * mean_importance) %>% 
  arrange(importance_weighted) %>% 
  #mutate(book = as.factor(book)) %>% 
  group_by(book) %>% 
  mutate(rank =  row_number()) %>% 
  filter(rank == 1) %>% 
  select(book, chapter, mean_importance, score, importance_weighted) %>% 
  arrange(book) %>% 
  knitr::kable(col.names = c("Book", "Chapter", "Mean importance", "Sentiment score",
                             "Weighted importance")) 
```

2-7 Heboric skullcup dessert treck
1-2 Pale
3-7 nothing
4-2 Karsa slavery
5-25 Climax
6-7 Battle of yghatan tunnel
7-9 A lot of gods and important characters meet
8-22 Many incredibly important characters meet separately
22-24 Part of the climax
9-15 A major leader dies
10-24 Final battle of the series

While all of these chapters may be considered climaxes (with the exception of Book 3 Chapter 7), I would say that only main climaxes are for boos 5, 8, and 10. Lets look at only the mean edge betweenness centrality on its own. 

```{r}
importance_df %>% 
  arrange(desc(mean_importance)) %>% 
  group_by(book) %>% 
  mutate(rank = row_number()) %>% 
  filter(rank == 1) %>% 
  arrange(book) %>% 
  select(-rank) %>% 
  knitr::kable(col.names = c("Book", "Chapter", "Mean Importance"))

# as_tbl_graph(network_data, directed = FALSE) %>% 
#   activate(edges) %>% 
#   filter(book == 9) %>% 
#   mutate(importance = centrality_edge_betweenness()) %>% 
#   as.tibble() %>% 
#   group_by(book, chapter) %>% 
#   summarize(mean_importance = mean(importance)) %>% 
#   arrange(desc(mean_importance))
```

1-6 Rake tries to make alliance with darhujistan
2-1 Nothing too special just   has a lot of dialogue
3-2 Tons of setup and foreshadowing
4-5 Karsa meets Leoman and joins shaik
5-22 Leads to climax
6-13 Not climax
7-13 nothing special
8-22 climax
9-13 nothing too special
10-26 Many of the main characters get their endings

The only main climax here is for Book 8, but the chapters for books 5 and 8 could be considered sub-climaxes. So just plain edge betweenness centrality doesn't quite work for finding the climaxes of each book. 

What about only using sentiment scores? 

```{r}
sentiment_scores %>% 
  arrange(score) %>% 
  group_by(book) %>% 
  mutate(rank = row_number()) %>% 
  filter(rank == 1) %>% 
  select(-rank) %>% 
  arrange(book) %>% 
  knitr::kable(col.names = c("Book", "Chapter", "Sentiment score"))
```

Well this appears to be the best metric by far for finding climaxes. A conclusion that can easily be drawn from this is that the most negative part of each book tends to be the climax in this series. Some of these chapters have major battles, revolutions, and a couple horribly morbid chapters. Books 3, 5, 7, 8, and 10 all have what I believe are the main climaxes in their respective books, and Books 2, 4, 6, and 15 are all sub-climaxes. 

While simple positive and negative sentiment scores are the best metrics for finding the climax of each book, the chapters with the highest mean edge betweenness centralities are very important chapters to the series as a whole. 

```{r}
importance_df %>% 
  arrange(desc(mean_importance)) %>% 
  head(10) %>% 
  knitr::kable(col.names = c("Book", "Chapter", "Mean importance"))
```

# Appendices
## Bibliography
https://www.reddit.com/r/Malazan/comments/a1ukxk/main_series_character_pov_data/

affinn

all malazan books

malazan wiki 

reddit people who helped make alias data

